{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#EXP 12 - NAME ENTITY RECOGNITION - LSTM"
      ],
      "metadata": {
        "id": "8Ab9bFsFaH3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "# Sample training data (replace this with your actual data)\n",
        "training_data = [\n",
        "    [(\"John\", \"B-PER\"), (\"Smith\", \"I-PER\"), (\"lives\", \"O\"), (\"in\", \"O\"), (\"New\", \"B-LOC\"), (\"York\", \"I-LOC\")],\n",
        "    [(\"Mary\", \"B-PER\"), (\"works\", \"O\"), (\"in\", \"O\"), (\"San\", \"B-LOC\"), (\"Francisco\", \"I-LOC\")]\n",
        "]\n",
        "\n",
        "# Create a vocabulary and labels set\n",
        "words = set()\n",
        "labels = set()\n",
        "for sentence in training_data:\n",
        "    for word, label in sentence:\n",
        "        words.add(word)\n",
        "        labels.add(label)\n",
        "\n",
        "# Assign indices to words and labels\n",
        "word2idx = {word: idx + 1 for idx, word in enumerate(words)}\n",
        "label2idx = {label: idx for idx, label in enumerate(labels)}\n",
        "\n",
        "# Convert sentences to sequences of indices\n",
        "X = [[word2idx[word] for word, _ in sentence] for sentence in training_data]\n",
        "y = [[label2idx[label] for _, label in sentence] for sentence in training_data]\n",
        "\n",
        "# Pad sequences to ensure equal length\n",
        "X = pad_sequences(X)\n",
        "y = pad_sequences(y)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y = [to_categorical(seq, num_classes=len(labels)) for seq in y]\n",
        "\n",
        "# Build the LSTM model with additional modifications\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(words) + 1, output_dim=100, input_length=X.shape[1]))\n",
        "model.add(Bidirectional(LSTM(units=200, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(LSTM(units=100, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(units=len(labels), activation='softmax'))\n",
        "\n",
        "# Compile the model with learning rate scheduling\n",
        "lr_schedule = ExponentialDecay(initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.9)\n",
        "model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on more epochs\n",
        "model.fit(X, np.array(y), epochs=30, batch_size=1)\n",
        "\n",
        "# Sample test data for prediction (replace this with your actual test data)\n",
        "test_sentence = [(\"John\",), (\"lives\",), (\"in\",), (\"New\",), (\"York\",)]\n",
        "\n",
        "# Convert test sentence to sequence of indices using the same word2idx mapping\n",
        "X_test = [[word2idx.get(word, 0) for word in test_sentence]]\n",
        "\n",
        "# Pad the sequence\n",
        "X_test = pad_sequences(X_test, maxlen=X.shape[1])\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Convert predictions to labels\n",
        "predicted_labels = [list(labels)[np.argmax(pred)] for pred in predictions[0]]\n",
        "\n",
        "# Print the results\n",
        "for (word, ), label in zip(test_sentence, predicted_labels):\n",
        "    print(f\"Word: {word}, Predicted Label: {label}\")\n"
      ],
      "metadata": {
        "id": "e7OHLhAaog-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing all the libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout,Bidirectional\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n"
      ],
      "metadata": {
        "id": "nzYQIWeppCuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trainign data is going to be a list of tuples, takek 2 sentences as 2 seperate lists of tupoles\n",
        "train=[\n",
        "     [(\"John\", \"B-PER\"), (\"Smith\", \"I-PER\"), (\"lives\", \"O\"), (\"in\", \"O\"), (\"New\", \"B-LOC\"), (\"York\", \"I-LOC\")],\n",
        "    [(\"Mary\", \"B-PER\"), (\"works\", \"O\"), (\"in\", \"O\"), (\"San\", \"B-LOC\"), (\"Francisco\", \"I-LOC\")]\n",
        "]"
      ],
      "metadata": {
        "id": "s_cbzP9Pua1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#put each of the wordsand labels into sets\n",
        "words=set()\n",
        "labels=set()\n",
        "for sentence in train:\n",
        "  for w,l in sentence:\n",
        "    words.add(w),\n",
        "    labels.add(l)\n"
      ],
      "metadata": {
        "id": "9wbBLoyrvKWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d8_UdltvnL4",
        "outputId": "1efe4df7-3a11-4fee-d4e1-037877eee8bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'York', 'John', 'lives', 'San', 'Francisco', 'in', 'New', 'Mary', 'works', 'Smith'}\n",
            "{'B-PER', 'O', 'B-LOC', 'I-PER', 'I-LOC'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#give each of these words indices\n",
        "words2idx={word:idx+1 for idx,word in enumerate(words)}\n",
        "labels2idx={label:idx for idx,label in enumerate(labels)}"
      ],
      "metadata": {
        "id": "NWrJiYqNvvcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words2idx)\n",
        "print(labels2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXuiqmwuwLmS",
        "outputId": "50eab9b5-3612-42f9-fb8c-c17bddd6c48a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'York': 1, 'John': 2, 'lives': 3, 'San': 4, 'Francisco': 5, 'in': 6, 'New': 7, 'Mary': 8, 'works': 9, 'Smith': 10}\n",
            "{'B-PER': 0, 'O': 1, 'B-LOC': 2, 'I-PER': 3, 'I-LOC': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert each of the sentences to sequences of indices\n",
        "x=[[words2idx[word] for word,_ in sentence] for sentence in train]\n",
        "y=[[labels2idx[word] for _,word in sentence] for sentence in train]"
      ],
      "metadata": {
        "id": "wNXmuGF6whYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAfuo8S_xvLf",
        "outputId": "0b601df6-b8e6-4041-85a7-e73d1f099fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 10, 3, 6, 7, 1], [8, 9, 6, 4, 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in train:\n",
        "  for word,_ in sentence:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYa9Dir4x6o4",
        "outputId": "b2293188-7882-4101-cd4f-ffa62e87b42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John\n",
            "Smith\n",
            "lives\n",
            "in\n",
            "New\n",
            "York\n",
            "Mary\n",
            "works\n",
            "in\n",
            "San\n",
            "Francisco\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=pad_sequences(x)\n",
        "y=pad_sequences(y)\n"
      ],
      "metadata": {
        "id": "BQ2nNiktyCUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=[to_categorical(val,num_classes=len(labels)) for val in y]"
      ],
      "metadata": {
        "id": "ABMCTqTDzYD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kWpKdD8zod6",
        "outputId": "93b4d8c5-3211-430f-ab4c-709cfd6cc3d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[1., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 1., 0., 0., 0.],\n",
            "       [0., 1., 0., 0., 0.],\n",
            "       [0., 0., 1., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.]], dtype=float32), array([[1., 0., 0., 0., 0.],\n",
            "       [1., 0., 0., 0., 0.],\n",
            "       [0., 1., 0., 0., 0.],\n",
            "       [0., 1., 0., 0., 0.],\n",
            "       [0., 0., 1., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.]], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=len(words)+1,output_dim=100, input_length=x.shape[1]))\n",
        "model.add(Bidirectional(LSTM(units=200,return_sequences=True,dropout=0.2, recurrent_dropout=0.2)))\n",
        "\n",
        "model.add(LSTM(units=100, return_sequences=True,dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(units=len(labels), activation='softmax'))\n",
        "\n",
        "lr_schedule = ExponentialDecay(initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.9)\n",
        "model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "11X20YbtzrNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x,np.array(y), epochs=50, batch_size=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sTLwXq_09Ok",
        "outputId": "eb81e664-41f7-4867-a0f2-f1392c23b91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "2/2 [==============================] - 8s 81ms/step - loss: 1.6062 - accuracy: 0.4167\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 1.5920 - accuracy: 0.3333\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 1.5753 - accuracy: 0.3333\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 1.5497 - accuracy: 0.3333\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 1.5141 - accuracy: 0.3333\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 77ms/step - loss: 1.4678 - accuracy: 0.3333\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 77ms/step - loss: 1.4054 - accuracy: 0.3333\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 84ms/step - loss: 1.3278 - accuracy: 0.3333\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 82ms/step - loss: 1.2731 - accuracy: 0.4167\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 90ms/step - loss: 1.1962 - accuracy: 0.5000\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 87ms/step - loss: 1.1045 - accuracy: 0.5000\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 80ms/step - loss: 0.9856 - accuracy: 0.5833\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 93ms/step - loss: 0.8656 - accuracy: 0.6667\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 84ms/step - loss: 0.7433 - accuracy: 0.7500\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.5968 - accuracy: 0.9167\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.5138 - accuracy: 0.9167\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 86ms/step - loss: 0.4311 - accuracy: 0.9167\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 84ms/step - loss: 0.3848 - accuracy: 0.9167\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 97ms/step - loss: 0.3284 - accuracy: 0.9167\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 87ms/step - loss: 0.2767 - accuracy: 0.9167\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.2570 - accuracy: 0.9167\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 82ms/step - loss: 0.2078 - accuracy: 0.9167\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 82ms/step - loss: 0.1937 - accuracy: 0.9167\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.1594 - accuracy: 0.9167\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 94ms/step - loss: 0.1977 - accuracy: 0.9167\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 0.1419 - accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 89ms/step - loss: 0.1528 - accuracy: 0.9167\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 93ms/step - loss: 0.1365 - accuracy: 0.9167\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 0.1273 - accuracy: 0.9167\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 95ms/step - loss: 0.1240 - accuracy: 0.9167\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 100ms/step - loss: 0.1011 - accuracy: 0.9167\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 80ms/step - loss: 0.0922 - accuracy: 0.9167\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 80ms/step - loss: 0.0779 - accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.0716 - accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.0656 - accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 85ms/step - loss: 0.0695 - accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 0.0692 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 85ms/step - loss: 0.0537 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 0.0591 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 154ms/step - loss: 0.0621 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.0445 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 174ms/step - loss: 0.0257 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 164ms/step - loss: 0.0349 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 165ms/step - loss: 0.0293 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 169ms/step - loss: 0.0253 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 131ms/step - loss: 0.0262 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 166ms/step - loss: 0.0272 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 244ms/step - loss: 0.0206 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 1s 285ms/step - loss: 0.0265 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 161ms/step - loss: 0.0185 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c9939cae530>"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = [(\"John\",), (\"lives\",), (\"in\",), (\"New\",), (\"York\",)]\n",
        "\n",
        "#converting this to sequences\n",
        "xtest=[[words2idx.get(word,0) for word in test_sentence]]\n",
        "xtest = pad_sequences(xtest, maxlen=x.shape[1])\n",
        "\n",
        "predictions=model.predict(xtest)\n",
        "\n",
        "prediction_label=[list(labels)[np.argmax(pred)] for pred in predictions[0]]\n",
        "print(prediction_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnWEpLu61IJT",
        "outputId": "9a450e45-9829-4cbb-fdd0-b9065b5ac64c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "['B-PER', 'B-PER', 'O', 'O', 'B-LOC', 'I-LOC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPO26wua1MWI",
        "outputId": "edc7d233-2cb2-46d6-cad5-021f19246fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[9.9061841e-01 2.4934110e-04 1.3082847e-04 8.9198891e-03 8.1635539e-05]\n",
            "  [6.4945531e-01 7.6214159e-03 5.6636095e-04 3.4226242e-01 9.4435360e-05]\n",
            "  [2.4554125e-04 9.9697709e-01 6.7720964e-04 2.0971613e-03 3.0159317e-06]\n",
            "  [1.2479983e-04 9.1122401e-01 8.8117555e-02 4.5829162e-04 7.5249030e-05]\n",
            "  [5.6584147e-05 9.4424631e-04 6.7289615e-01 1.0659680e-04 3.2599637e-01]\n",
            "  [1.4043136e-06 1.0932634e-06 4.8692862e-04 8.2960378e-07 9.9950969e-01]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECI6Vv9r5ssQ",
        "outputId": "fc9611cc-27b0-4568-fb83-2468f9d36968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 6, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XputaubM51lu",
        "outputId": "d3747ea4-b5e2-4f57-99f3-62696af815d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.9061841e-01 2.4934110e-04 1.3082847e-04 8.9198891e-03 8.1635539e-05]\n",
            " [6.4945531e-01 7.6214159e-03 5.6636095e-04 3.4226242e-01 9.4435360e-05]\n",
            " [2.4554125e-04 9.9697709e-01 6.7720964e-04 2.0971613e-03 3.0159317e-06]\n",
            " [1.2479983e-04 9.1122401e-01 8.8117555e-02 4.5829162e-04 7.5249030e-05]\n",
            " [5.6584147e-05 9.4424631e-04 6.7289615e-01 1.0659680e-04 3.2599637e-01]\n",
            " [1.4043136e-06 1.0932634e-06 4.8692862e-04 8.2960378e-07 9.9950969e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rG6pHZ158jM",
        "outputId": "db9d1a86-abe6-484d-f7c2-119e4450066e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'B-PER', 'O', 'B-LOC', 'I-PER', 'I-LOC'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in predictions[0]:\n",
        "  print(np.argmax(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixJV7vmc6F37",
        "outputId": "56967706-057d-4e26-94a2-ca38db929bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "2\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wJO2vC0U6NRV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}